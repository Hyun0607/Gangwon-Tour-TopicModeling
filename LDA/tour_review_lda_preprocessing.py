# -*- coding: utf-8 -*-
"""tour_review_lda_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ymAjDvzON5IRy5xDPbOZphrwgr4USHc9
"""

!pip install konlpy
!apt-get -y install fonts-nanum # 폰트 이슈 방지 (옵션)

# -*- coding: utf-8 -*-
# tour_review_lda_preprocessing.py

import re
import csv
import pandas as pd
from pandas import DataFrame
from tqdm import tqdm
from konlpy.tag import Mecab

# ✅ 불용어 리스트 정의
stop_words = ['시간', '추천', '느낌', '최고', '방문', '생각', '정도', '입장료', '장소',
              '스팟', '날씨', '마지막', '가격', '가지', '사람', '가능', '코로나', '이용',
              '이곳', '위치', '여기', '기분', '우리', '하나', '주차', '주차장', '남이섬']

# ✅ 텍스트 정제 함수
def clean_text(text):
    if isinstance(text, str):
        text = text.replace(".", "").strip()
        text = text.replace("·", " ").strip()
        pattern = '[^ ㄱ-ㅣ가-힣|0-9]+'
        return re.sub(pattern=pattern, repl='', string=text)
    else:
        return ""

# ✅ 명사 추출 함수 (konlpy 기반)
def get_nouns(tokenizer, sentence):
    nouns = tokenizer.nouns(sentence)
    return [noun for noun in nouns if len(noun) > 1]

# ✅ 불용어 제거 함수
def remove_stopwords(word_list, stop_words):
    return [word for word in word_list if word not in stop_words]

# ✅ 리뷰 데이터 전체 전처리
def preprocess_reviews(df, stop_words):
    tokenizer = Mecab()
    processed_reviews = []
    for review in df['리뷰']:
        cleaned = clean_text(str(review))
        nouns = get_nouns(tokenizer, cleaned)
        filtered = remove_stopwords(nouns, stop_words)
        processed_reviews.append(filtered)
    df['불용어 제거 후'] = processed_reviews
    return df

# ✅ 토큰화 함수
def tokenize(df, stop_words):
    tokenizer = Mecab()
    processed_data = []
    for sent in tqdm(df['리뷰']):
        sentence = clean_text(str(sent).replace("\n", "").strip())
        nouns = get_nouns(tokenizer, sentence)
        filtered_nouns = remove_stopwords(nouns, stop_words)
        processed_data.append(filtered_nouns)
    return processed_data

# ✅ 토큰화된 데이터 저장 함수
def save_processed_data(processed_data, filename="tokenized_data.txt"):
    with open(filename, 'w', newline="", encoding='utf-8') as f:
        writer = csv.writer(f)
        for data in processed_data:
            writer.writerow(data)

# ✅ 메인 실행
if __name__ == '__main__':
    df = pd.read_csv("남이섬.csv")
    df.columns = ['장소', '리뷰']
    df.dropna(how='any', inplace=True)

    df = preprocess_reviews(df, stop_words)
    processed_data = tokenize(df, stop_words)
    save_processed_data(processed_data)